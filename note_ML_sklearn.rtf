{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1036{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\colortbl ;\red0\green0\blue255;\red192\green192\blue192;}
{\*\generator Riched20 10.0.10586}\viewkind4\uc1 
\pard\sa200\sl276\slmult1\f0\fs28\lang12 Udacity: Introduction of Machine Learning\par
 git clone {{\field{\*\fldinst{HYPERLINK https://github.com/udacity/ud120-projects.git }}{\fldrslt{https://github.com/udacity/ud120-projects.git\ul0\cf0}}}}\f0\fs28\par
Documentation: Scikit Learn\par
\b Naive Bayes\par
{\fs24{\field{\*\fldinst{HYPERLINK http://scikit-learn.org/stable/modules/naive_bayes.html }}{\fldrslt{http://scikit-learn.org/stable/modules/naive_bayes.html\ul0\cf0}}}}\f0\fs24\par
\b0 Naive Bayes methods are a set of supervised learning algorithms based on applying Bayes\rquote  theorem with the \ldblquote naive\rdblquote  assumption of independence between every pair of features.\par
P(y|x1, x2, ..., xn) = P(y)*P(x1, x2, ..., xn|y)/P(x1, x2, ..., xn)\par
Using the naive independence assumption that\par
P(xi|y, x1, ..., xi-1, xi+1, ..., xn) = P(xi|y)\par
for all i, this relationship is simplified to\par
P(y|x1, x2, ..., xn) = P(y)*sum(P(xi|y))/P(x1, x2, ..., xn)\par
Since P(x1, x2, ..., xn) is constant given the input, we can use the following classification rule:\par
P(y)*P(x1, x2, ..., xn|y) => P(y)*sum(P(xi|y))\par
so Y' = arg Max P(y)*sum(P(xi|y))\par
and we can use Maximum A Posteriori (MAP) estimation to estimate P(y) and P(xi|y); the former is then the relative frequency of class y in the training set. Y' is the class to be decided.\par
The \b different naive Bayes classifiers \b0 differ mainly by the \b assumptions they make regarding the distribution of P(xi |y)\par
\b0 In spite of their apparently \b over-simplified assumptions\b0 , naive Bayes classifiers have worked quite well in many real-world situations\b , famously document classification and spam filtering\b0 . They require a \b small amount of training data to estimate the necessary parameters\b0 . (For theoretical reasons why naive Bayes works well, and on which types of data it does, see the references below.)\par
On the flip side, although naive Bayes is known as a decent classifier, it is known to be a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\par
\b GaussianNB \b0 implements the Gaussian Naive Bayes algorithm for classification. The likelihood of the features is assumed to be Gaussian:\par
P(xi|y) = 1/sqart(2*pi*theta\sub y\super 2\nosupersub )exp(-(x\sub i\nosupersub -\'b5\sub y\nosupersub )\super 2\nosupersub /2*theta\sub y\super 2\nosupersub )\par
\b MultinomialNB\b0  implements the naive Bayes algorithm \b for multinomially distributed\b0  \b data\b0 , and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice). The distribution is parametrized by vectors \\theta_y = (\\theta_\{y1\},\\ldots,\\theta_\{yn\}) for each class y, \b where n is the number of features (in text classification, the size of the vocabulary)\b0  and \\theta_\{yi\} is the probability P(x_i \\mid y) of feature i appearing in a sample belonging to class y.\par
\b BernoulliNB \b0 implements the naive Bayes training and classification algorithms for data that is distributed according to multivariate Bernoulli distributions; i.e., there may be multiple features but \b each one is assumed to be a binary-valued \b0 (Bernoulli, boolean) variable. Therefore, \b this class requires samples to be represented as binary-valued feature vectors; \b0 if handed any other kind of data, a BernoulliNB instance may binarize its input (depending on the binarize parameter).\par
The decision rule for Bernoulli naive Bayes is based on\par
P(x_i \\mid y) = P(i \\mid y) x_i + (1 - P(i \\mid y)) (1 - x_i)\par
which differs from multinomial NB\rquote s rule in \b that it explicitly penalizes the non-occurrence of a feature i \b0 that is an indicator for class y, where the \b multinomial variant would simply ignore a non-occurring feature.\b0\par
In the case of text classification, word occurrence vectors (rather than word count vectors) may be used to train and use this classifier. BernoulliNB might perform better on some datasets, especially those with shorter documents. It is advisable to evaluate both models, if time permits.\par
Naive Bayes models can be used to tackle large scale classification problems for which the full training set might not fit in memory. To handle this case, MultinomialNB, BernoulliNB, and GaussianNB expose a partial_fit method that can be used incrementally as done with other classifiers as demonstrated in Out-of-core classification of text documents. All naive Bayes classifiers support sample weighting.\par
Contrary to the fit method, the first call to partial_fit needs to be passed the list of all the expected class labels.\par
For an overview of available strategies in scikit-learn, see also the out-of-core learning documentation.\par
Difference between the three naive bayes:\par
\highlight2 from sklearn import datasets\par
 iris = datasets.load_iris()\par
 from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\par
 gnb = GaussianNB()\par
 y_pred = gnb.fit(iris.data, iris.target).predict(iris.data)\par
 print("Number of mislabeled points out of a total %d points : %d" % (iris.data.shape[0],(iris.target != y_pred).sum()))\par
 mnb = MultinomialNB()\par
 y_pred_mnb = mnb.fit(iris.data, iris.target).predict(iris.data)\par
 print("Number of mislabeled points out of a total %d points : %d" % (iris.data.shape[0],(iris.target != y_pred_mnb).sum()))\par
 bnb = BernoulliNB()\par
 y_pred_bnb = bnb.fit(iris.data, iris.target).predict(iris.data)\par
 print("Number of mislabeled points out of a total %d points : %d" % (iris.data.shape[0],(iris.target !=    y_pred_bnb).sum()))\par
\highlight0\par
In a nutshell, the \b Gaussian Naive Bayes \b0 model is generally used for \b continuous data (where each feature is a real number), where the underlying data distribution is assumed to be a Gaussian (Normal) distribution\b0 .\par
The \b Multinomial Naive Bayes \b0 model counts \b how often a certain event occurs \b0 in the dataset (for example how \b often a certain word occurs in a document\b0 ).\par
The \b Bernoulli Naive Bayes \b0 model is similar to the Multinomial Naive Bayes model, but instead of counting how often an event occurred, \b it only describes whether or not an event occurred\b0  (for example whether or not \b a certain word occurs in a document, where it doesn't matter if it occurs once or 100000 times\b0 )\par
Now specifically for the iris dataset, which \b contains real valued data (not frequency !)\b0 , the \b GaussianNB would be the most suitable model\b0 . A classic example of where the MultinomialNB would be most appropriate is Text classification.\par
So given the characteristics of your dataset and each model, it is not surprising that the Bernoulli Naive Bayes model didn't do very well\b . The larger surprise is that the Multinomial Naive Bayes model did almost as well as the Gaussian model even though its an inappropriate choice for this dataset (real valued data)\b0 . However looking at the data (I encourage you to do it) should give you the right hint why the Multinomial Naive Bayes model could achieve such a good performance.\par
For an in-depth comparison of the \b Bernoulli Naive Bayes model and the Multinomial Naive Bayes model for Text classification \b0 I would suggest you read this paper.\par
You might also want to check the excellent scikit-learn documentation on Naive Bayes for an overview of the different models, and last but not least, the wikipedia page on the Naive Bayes Classifier gives an excellent overview as well.\par
\par
\b\fs28 Support Vector Machines (SVM)\b0\fs24\par
a set of supervised learning methods used for \b classification\b0 , \b regression \b0 and \b outliers detection\b0 .\par
\ul The advantages of support vector machines are:\par
\ulnone\b Effective \b0 in \b high dimensional \b0 spaces.\par
Still effective in cases where \b number of dimensions is greater than the number of samples.\par
\b0 Uses a \b subset of training points \b0 in \b the decision function \b0 (called \b support vectors\b0 ), so it is also memory efficient.\par
Versatile: \b different Kernel functions \b0 can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\par
\ul The disadvantages of support vector machines include:\par
\ulnone If the number of features is much greater than the number of samples, the method is likely to give poor performances.\par
SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\par
The support vector machines in scikit-learn support both dense (numpy.ndarray and convertible to that by numpy.asarray) and sparse (any scipy.sparse) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered numpy.ndarray (dense) or scipy.sparse.csr_matrix (sparse) with dtype=float64.\par
\b SVC\b0 , \b NuSVC \b0 and \b LinearSVC \b0 are classes capable of performing multi-class classification on a dataset.\par
\b SVC\b0  and \b NuSVC\b0  are similar methods, but accept slightly different sets of parameters and have different mathematical formulations (see section Mathematical formulation). \par
On the other hand, \b LinearSVC \b0 is another implementation of Support Vector Classification for the case of a linear kernel. Note that LinearSVC does not accept keyword kernel, as this is assumed to be linear. It also lacks some of the members of SVC and NuSVC, like support_.\par
\b SVMs \b0 decision function depends on some subset of the \b training data, called the support vectors\b0 . Some properties of these support vectors can be found in members support_vectors_, support_ and n_support:\par
\highlight2 >>> # get support vectors\par
>>> clf.support_vectors_\par
array([[ 0.,  0.],\par
       [ 1.,  1.]])\par
>>> # get indices of support vectors\par
>>> clf.support_ \par
array([0, 1]...)\par
>>> # get number of support vectors for each class\par
>>> clf.n_support_ \par
array([1, 1]...)\par
\par
\highlight0 SVC and NuSVC implement \b the \ldblquote one-against-one\rdblquote  \b0 approach (Knerr et al., 1990) for multi- class classification. If n_class is the number of classes, then n_class * (n_class - 1) / 2 classifiers are constructed and each one trains data from two classes. To provide a consistent interface with other classifiers, the \b decision_function_shape \b0 option allows to aggregate the results of the \ldblquote one-against-one\rdblquote  classifiers to a decision function of shape (n_samples, n_classes):\par
\highlight2 >>> X = [[0], [1], [2], [3]]\par
>>> Y = [0, 1, 2, 3]\par
>>> clf = svm.SVC(decision_function_shape='ovo')\par
>>> clf.fit(X, Y) \par
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\par
    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\par
    max_iter=-1, probability=False, random_state=None, shrinking=True,\par
    tol=0.001, verbose=False)\par
>>> dec = clf.decision_function([[1]])\par
>>> dec.shape[1] # 4 classes: 4*3/2 = 6\par
6\par
>>> clf.decision_function_shape = "ovr"\par
>>> dec = clf.decision_function([[1]])\par
>>> dec.shape[1] # 4 classes\par
4\par
\highlight0 On the other hand, LinearSVC implements \b\ldblquote one-vs-the-rest\rdblquote\b0  multi-class strategy, thus training n_class models. If there are only two classes, only one model is trained:\par
\highlight2 >>> lin_clf = svm.LinearSVC()\par
>>> lin_clf.fit(X, Y) \par
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\par
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\par
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\par
     verbose=0)\par
>>> dec = lin_clf.decision_function([[1]])\par
>>> dec.shape[1]\par
4\par
\par
\highlight0 In problems where it is desired to give more importance to certain classes or certain individual samples keywords class_weight and sample_weight can be used.\par
SVC (but not NuSVC) implement a keyword class_weight in the fit method. It\rquote s a dictionary of the form \{class_label : value\}, where value is a floating point number > 0 that sets the parameter C of class class_label to C * value.\par
\par
The method of Support Vector Classification can be extended to solve \b regression \b0 problems. This method is called \b Support Vector Regression\b0 .\par
The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction.\par
\par
1.4. Support Vector Machines\par
Support vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\par
The advantages of support vector machines are:\par
Effective in high dimensional spaces.\par
Still effective in cases where number of dimensions is greater than the number of samples.\par
Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\par
Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\par
The disadvantages of support vector machines include:\par
If the number of features is much greater than the number of samples, the method is likely to give poor performances.\par
SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation (see Scores and probabilities, below).\par
The support vector machines in scikit-learn support both dense (numpy.ndarray and convertible to that by numpy.asarray) and sparse (any scipy.sparse) sample vectors as input. However, to use an SVM to make predictions for sparse data, it must have been fit on such data. For optimal performance, use C-ordered numpy.ndarray (dense) or scipy.sparse.csr_matrix (sparse) with dtype=float64.\par
1.4.1. Classification\par
\par
SVC, NuSVC and LinearSVC are classes capable of performing multi-class classification on a dataset.\par
../_images/sphx_glr_plot_iris_0012.png\par
SVC and NuSVC are similar methods, but accept slightly different sets of parameters and have different mathematical formulations (see section Mathematical formulation). On the other hand, LinearSVC is another implementation of Support Vector Classification for the case of a linear kernel. Note that LinearSVC does not accept keyword kernel, as this is assumed to be linear. It also lacks some of the members of SVC and NuSVC, like support_.\par
As other classifiers, SVC, NuSVC and LinearSVC take as input two arrays: an array X of size [n_samples, n_features] holding the training samples, and an array y of class labels (strings or integers), size [n_samples]:\par
>>>\par
>>> from sklearn import svm\par
>>> X = [[0, 0], [1, 1]]\par
>>> y = [0, 1]\par
>>> clf = svm.SVC()\par
>>> clf.fit(X, y)  \par
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\par
    decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\par
    max_iter=-1, probability=False, random_state=None, shrinking=True,\par
    tol=0.001, verbose=False)\par
After being fitted, the model can then be used to predict new values:\par
>>>\par
>>> clf.predict([[2., 2.]])\par
array([1])\par
SVMs decision function depends on some subset of the training data, called the support vectors. Some properties of these support vectors can be found in members support_vectors_, support_ and n_support:\par
>>>\par
>>> # get support vectors\par
>>> clf.support_vectors_\par
array([[ 0.,  0.],\par
       [ 1.,  1.]])\par
>>> # get indices of support vectors\par
>>> clf.support_ \par
array([0, 1]...)\par
>>> # get number of support vectors for each class\par
>>> clf.n_support_ \par
array([1, 1]...)\par
1.4.1.1. Multi-class classification\par
\par
SVC and NuSVC implement the \ldblquote one-against-one\rdblquote  approach (Knerr et al., 1990) for multi- class classification. If n_class is the number of classes, then n_class * (n_class - 1) / 2 classifiers are constructed and each one trains data from two classes. To provide a consistent interface with other classifiers, the decision_function_shape option allows to aggregate the results of the \ldblquote one-against-one\rdblquote  classifiers to a decision function of shape (n_samples, n_classes):\par
>>>\par
>>> X = [[0], [1], [2], [3]]\par
>>> Y = [0, 1, 2, 3]\par
>>> clf = svm.SVC(decision_function_shape='ovo')\par
>>> clf.fit(X, Y) \par
SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\par
    decision_function_shape='ovo', degree=3, gamma='auto', kernel='rbf',\par
    max_iter=-1, probability=False, random_state=None, shrinking=True,\par
    tol=0.001, verbose=False)\par
>>> dec = clf.decision_function([[1]])\par
>>> dec.shape[1] # 4 classes: 4*3/2 = 6\par
6\par
>>> clf.decision_function_shape = "ovr"\par
>>> dec = clf.decision_function([[1]])\par
>>> dec.shape[1] # 4 classes\par
4\par
On the other hand, LinearSVC implements \ldblquote one-vs-the-rest\rdblquote  multi-class strategy, thus training n_class models. If there are only two classes, only one model is trained:\par
>>>\par
>>> lin_clf = svm.LinearSVC()\par
>>> lin_clf.fit(X, Y) \par
LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\par
     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\par
     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\par
     verbose=0)\par
>>> dec = lin_clf.decision_function([[1]])\par
>>> dec.shape[1]\par
4\par
See Mathematical formulation for a complete description of the decision function.\par
Note that the LinearSVC also implements an alternative multi-class strategy, the so-called multi-class SVM formulated by Crammer and Singer, by using the option multi_class='crammer_singer'. This method is consistent, which is not true for one-vs-rest classification. In practice, one-vs-rest classification is usually preferred, since the results are mostly similar, but the runtime is significantly less.\par
For \ldblquote one-vs-rest\rdblquote  LinearSVC the attributes coef_ and intercept_ have the shape [n_class, n_features] and [n_class] respectively. Each row of the coefficients corresponds to one of the n_class many \ldblquote one-vs-rest\rdblquote  classifiers and similar for the intercepts, in the order of the \ldblquote one\rdblquote  class.\par
In the case of \ldblquote one-vs-one\rdblquote  SVC, the layout of the attributes is a little more involved. In the case of having a linear kernel, The layout of coef_ and intercept_ is similar to the one described for LinearSVC described above, except that the shape of coef_ is [n_class * (n_class - 1) / 2, n_features], corresponding to as many binary classifiers. The order for classes 0 to n is \ldblquote 0 vs 1\rdblquote , \ldblquote 0 vs 2\rdblquote  , ... \ldblquote 0 vs n\rdblquote , \ldblquote 1 vs 2\rdblquote , \ldblquote 1 vs 3\rdblquote , \ldblquote 1 vs n\rdblquote , . . . \ldblquote n-1 vs n\rdblquote .\par
The shape of dual_coef_ is [n_class-1, n_SV] with a somewhat hard to grasp layout. The columns correspond to the support vectors involved in any of the n_class * (n_class - 1) / 2 \ldblquote one-vs-one\rdblquote  classifiers. Each of the support vectors is used in n_class - 1 classifiers. The n_class - 1 entries in each row correspond to the dual coefficients for these classifiers.\par
This might be made more clear by an example:\par
Consider a three class problem with class 0 having three support vectors v^\{0\}_0, v^\{1\}_0, v^\{2\}_0 and class 1 and 2 having two support vectors v^\{0\}_1, v^\{1\}_1 and v^\{0\}_2, v^\{1\}_2 respectively. For each support vector v^\{j\}_i, there are two dual coefficients. Let\rquote s call the coefficient of support vector v^\{j\}_i in the classifier between classes i and k \\alpha^\{j\}_\{i,k\}. Then dual_coef_ looks like this:\par
\\alpha^\{0\}_\{0,1\}\tab\\alpha^\{0\}_\{0,2\}\tab Coefficients for SVs of class 0\par
\\alpha^\{1\}_\{0,1\}\tab\\alpha^\{1\}_\{0,2\}\par
\\alpha^\{2\}_\{0,1\}\tab\\alpha^\{2\}_\{0,2\}\par
\\alpha^\{0\}_\{1,0\}\tab\\alpha^\{0\}_\{1,2\}\tab Coefficients for SVs of class 1\par
\\alpha^\{1\}_\{1,0\}\tab\\alpha^\{1\}_\{1,2\}\par
\\alpha^\{0\}_\{2,0\}\tab\\alpha^\{0\}_\{2,1\}\tab Coefficients for SVs of class 2\par
\\alpha^\{1\}_\{2,0\}\tab\\alpha^\{1\}_\{2,1\}\par
1.4.1.2. Scores and probabilities\par
\par
The SVC method decision_function gives per-class scores for each sample (or a single score per sample in the binary case). When the constructor option probability is set to True, class membership probability estimates (from the methods predict_proba and predict_log_proba) are enabled. In the binary case, the probabilities are calibrated using Platt scaling: logistic regression on the SVM\rquote s scores, fit by an additional cross-validation on the training data. In the multiclass case, this is extended as per Wu et al. (2004).\par
Needless to say, the cross-validation involved in Platt scaling is an expensive operation for large datasets. In addition, the probability estimates may be inconsistent with the scores, in the sense that the \ldblquote argmax\rdblquote  of the scores may not be the argmax of the probabilities. (E.g., in binary classification, a sample may be labeled by predict as belonging to a class that has probability <\'bd according to predict_proba.) Platt\rquote s method is also known to have theoretical issues. If confidence scores are required, but these do not have to be probabilities, then it is advisable to set probability=False and use decision_function instead of predict_proba.\par
References:\par
Wu, Lin and Weng, \ldblquote Probability estimates for multi-class classification by pairwise coupling\rdblquote , JMLR 5:975-1005, 2004.\par
1.4.1.3. Unbalanced problems\par
\par
In problems where it is desired to give more importance to certain classes or certain individual samples keywords class_weight and sample_weight can be used.\par
SVC (but not NuSVC) implement a keyword class_weight in the fit method. It\rquote s a dictionary of the form \{class_label : value\}, where value is a floating point number > 0 that sets the parameter C of class class_label to C * value.\par
../_images/sphx_glr_plot_separating_hyperplane_unbalanced_0011.png\par
SVC, NuSVC, SVR, NuSVR and OneClassSVM implement also weights for individual samples in method fit through keyword sample_weight. Similar to class_weight, these set the parameter C for the i-th example to C * sample_weight[i].\par
../_images/sphx_glr_plot_weighted_samples_0011.png\par
Examples:\par
Plot different SVM classifiers in the iris dataset,\par
SVM: Maximum margin separating hyperplane,\par
SVM: Separating hyperplane for unbalanced classes\par
SVM-Anova: SVM with univariate feature selection,\par
Non-linear SVM\par
SVM: Weighted samples,\par
1.4.2. Regression\par
\par
The method of Support Vector Classification can be extended to solve regression problems. This method is called Support Vector Regression.\par
The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by Support Vector Regression depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction.\par
There are three different implementations of Support Vector Regression: SVR, NuSVR and LinearSVR. LinearSVR provides a faster implementation than SVR but only considers linear kernels, while NuSVR implements a slightly different formulation than SVR and LinearSVR. See Implementation details for further details.\par
\b One-class SVM \b0 is used for \b novelty detection\b0 , that is, given a set of samples, it will detect the \b soft boundary of that set so as to classify new points as belonging to that set or not\b0 . The class that implements this is called OneClassSVM.\par
In this case, as it is a type of unsupervised learning, the fit method will only take as input an array X, as there are no class labels.\par
Support Vector Machines are powerful tools, but their compute and storage requirements increase rapidly with the number of training vectors. The core of an SVM is a \b quadratic programming problem (QP), \b0 separating support vectors from the rest of the training data. The QP solver used by this libsvm-based implementation scales between O(n_\{features\} \\times n_\{samples\}^2) and O(n_\{features\} \\times n_\{samples\}^3) depending on how efficiently the \b libsvm \b0 cache is used in practice (dataset dependent). If the data is very sparse n_\{features\} should be replaced by the average number of non-zero features in a sample vector.\par
Also note that for the linear case, the algorithm used \b in LinearSVC \b0 by the \b liblinear \b0 implementation is much more efficient than its libsvm-based SVC counterpart and can scale almost linearly to millions of samples and/or features.\par
\b Tips on Practical Use\b0\par
Kernel cache size: For SVC, SVR, nuSVC and NuSVR, the size of the kernel cache has a strong impact on run times for larger problems\b . If you have enough RAM available, it is recommended to set cache_size to a higher value than the default of 200(MB), such as 500(MB) or 1000(MB).\par
\b0 Setting C: \b C is 1 by default and it\rquote s a reasonable default choice. \b0 If you have a lot of noisy observations you should decrease it. It corresponds to regularize more the estimation.\par
\b Support Vector Machine algorithms are not scale invariant, so it is highly recommended to scale your data. \b0 For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the same scaling must be applied to the test vector to obtain meaningful results.\par
In SVC, if data for classification are unbalanced (e.g. many positive and few negative), set class_weight='balanced' and/or try different penalty parameters C.\par
\b Radial Basis Function (RBF) kernel\par
\b0 two parameters must be considered: C and gamma. \b The parameter C, common to all SVM kernels, trades off misclassification of training examples against simplicity of the decision surface.\b0  A low C makes the decision surface smooth, \b while a high C aims at classifying all training examples correctly. \b0 gamma defines how much influence a single training example has. The larger gamma is, the closer other examples must be to be affected.\par
Proper choice of C and gamma is critical to the SVM\rquote s performance. One is advised to use sklearn.model_selection.GridSearchCV with C and gamma spaced exponentially far apart to choose good values.\par
gram = np.dot(X, X.T) ?????\par
\b\fs28 Nearest Neighbors\par
\b0\fs24 The principle behind nearest neighbor methods is to find a \b predefined number of training samples closest in distance\b0  to the new point, and predict the label from these. The number of samples can be a \b user-defined constant \b0 (\b k-nearest neighbor learning\b0 ), or vary based on the \b local density of points\b0  (\b radius-based neighbor learning\b0 ). The distance can, in general, be any metric measure: standard Euclidean distance is the most common choice. The distance can, in general, be any metric measure: \b standard Euclidean distance \b0 is the most common choice. \b Neighbors-based methods\b0  are known \b as non-generalizing machine learning methods\b0 , since they simply \ldblquote remember\rdblquote  all of its training data (possibly transformed into a fast indexing structure such as a Ball Tree or KD Tree.).\par
The classes in sklearn.neighbors can handle either Numpy arrays or scipy.sparse matrices as input. For dense matrices, a large number of possible distance metrics are supported. For sparse matrices, arbitrary Minkowski metrics are supported for searches.\par
\b NearestNeighbors\b0  implements \b unsupervised nearest neighbors learning\b0 . It acts as a \b uniform interface \b0 to three different nearest neighbors algorithms: BallTree, KDTree, and a brute-force algorithm based on routines in sklearn.metrics.pairwise. The choice of neighbors search algorithm is controlled through the keyword 'algorithm', which must be one of ['auto', 'ball_tree', 'kd_tree', 'brute'].\par
\b\fs28 Nearest Neighbors Classification\par
\b0\fs24 Neighbors-based classification is a type of \b instance-based learning\b0  or \b non-generalizing learning\b0 : it does not attempt to construct a general internal model, but simply stores instances of the training data\b . Classification is computed from a simple majority vote of the nearest neighbors of each point\b0 : a query point is assigned the data class which has the most representatives within the nearest neighbors of the point.\par
scikit-learn implements two different nearest neighbors classifiers: \b KNeighborsClassifier\b0  implements learning based on \b the  k nearest neighbors of each query point\b0 , where k is an integer value specified by the user. \b RadiusNeighborsClassifier\b0  implements learning based on the number of neighbors within \b a fixed radius r\b0  of each training point, where \b r is a floating-point value\b0  specified by the user.\par
The basic nearest neighbors classification uses \b uniform weights\b0 : that is, the value assigned to a query point \b is computed from a simple majority vote of the nearest neighbors\b0 . Under some circumstances, it is better to weight the neighbors such that nearer neighbors contribute more to the fit. This can be accomplished through the weights keyword. The default value\b , weights = 'uniform', assigns uniform weights to each neighbor\b0 . \b weights = 'distance' assigns weights proportional to the inverse of the distance from the query point\b0 . Alternatively, a user-defined function of the distance can be supplied which is used to compute the weights.\par
\b\fs28 Nearest Neighbors Regression\par
Nearest Neighbor Algorithms\par
\fs24 Brute Force\fs28\par
\b0\fs24 Fast computation of nearest neighbors is an active area of research in machine learning\b . The most naive neighbor search implementation involves the brute-force computation of distances between all pairs of points in the dataset\b0 : for N samples in  D dimensions, this approach scales as O[D N^2]. Efficient brute-force neighbors searches can be very competitive for \b small data samples\b0 . However, as the number of samples N grows, the brute-force approach quickly becomes infeasible. \par
 \b K-D Tree\par
\b0 To address the computational inefficiencies of the brute-force approach, a variety of tree-based data structures have been invented. In general, these structures attempt \b to reduce the required number of distance calculations \b0 by efficiently encoding aggregate distance information for the sample. The basic idea is that if point A is very distant from point B, and point B is very close to point C, then we know that points A and C are very distant, without having to explicitly calculate their distance. \par
An early approach to taking advantage of this aggregate information was the KD tree data structure (short for \b K-dimensional tree\b0 ), which generalizes two-dimensional Quad-trees and 3-dimensional Oct-trees to an arbitrary number of dimensions. The KD tree is a binary tree structure which recursively partitions the parameter space along the data axes, dividing it into nested orthotopic regions into which data points are filed. The construction of a KD tree is very fast: because partitioning is performed only along the data axes, no D-dimensional distances need to be computed. Once constructed, the nearest neighbor of a query point can be determined with only O[\\log(N)] distance computations. Though the \b KD tree approach is very fast for low-dimensional (D < 20) neighbors searches\b0 , it becomes inefficient as D grows very large: this is one manifestation of the so-called \ldblquote curse of dimensionality\rdblquote . In scikit-learn, KD tree neighbors searches are specified using the keyword algorithm = 'kd_tree', and are computed using the class KDTree.\par
\b Ball Tree\par
\b0 To address the inefficiencies of KD Trees in higher dimensions, the ball tree data structure was developed. Where KD trees partition data along Cartesian axes, ball trees partition data in a \b series of nesting hyper-spheres\b0 . This makes tree construction more costly than that of the KD tree, but results in a data structure which can be very efficient on highly-structured data, even in very high dimensions.\par
\b A ball tree recursively divides the data into nodes defined by a centroid C and radius r, such that each point in the node lies within the hyper-sphere defined by r and C.\b0  The number of candidate points for a neighbor search is reduced through use of the triangle inequality:\par
|x+y| \\leq |x| + |y|\par
With this setup, a single distance \b calculation between a test point and the centroid is sufficient to determine a lower and upper bound on the distance to all points within the node.\b0  Because of the spherical geometry of the ball tree nodes, it can out-perform a KD-tree in high dimensions, though the actual performance is highly dependent on the structure of the training data. \par
\par
\par
\par
}
 